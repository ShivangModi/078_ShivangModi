{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "078_07_01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbW-zJ3SMe1g"
      },
      "source": [
        "**Replace Manual version of Logistic Regression with TF based version.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAS6wk9zwYLQ",
        "outputId": "c4932460-b407-4a18-ab97-22754124579f"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us2znOKGxDgQ",
        "outputId": "0d04a861-40d8-46e6-efac-f4cd6921c584"
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQPinS2TxRIE"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuAD9QvPxe37"
      },
      "source": [
        "Pre-processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa9TsNpYxbwH"
      },
      "source": [
        "# cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    \n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    \n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    \n",
        "    # remove hashtags: only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    \n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if word not in stopwords_english and word not in string.punctuation:\n",
        "            stem_word = stemmer.stem(word)\n",
        "            tweets_clean.append(stem_word)\n",
        "    return tweets_clean"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swBO1CGLyier"
      },
      "source": [
        "# a dictionary mapping each (word, sentiment) pair to its frequency\n",
        "def calc_freqs(tweets, ys):\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "    \n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet=tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "    return freqs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1q2je_OyxcT"
      },
      "source": [
        "Split the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoLHQdkQy02W"
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
        "all_negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
        "\n",
        "# spliting of the data\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "x_test = test_pos + test_neg\n",
        "\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "x_train = train_pos + train_neg\n",
        "\n",
        "# similarly create labels of training and testing data\n",
        "y_train = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "y_test = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
        "freqs = calc_freqs(x_train, y_train)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8sgaPqLzp2C"
      },
      "source": [
        "# x: a feature vector of dimension (1,2)\n",
        "def extract_features(tweet, freqs):\n",
        "    word_l =  process_tweet(tweet)\n",
        "    x = np.zeros((1, 2)) \n",
        "    \n",
        "    for word in word_l:\n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,0] += freqs.get((word, 1.0),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,1] += freqs.get((word, 0.0),0)\n",
        "    \n",
        "    assert(x.shape == (1, 2))\n",
        "    return x"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWnp3_ik0Efc"
      },
      "source": [
        "Create model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcpG58lZz-yO"
      },
      "source": [
        "class TweetLogisticRegressionModel:\n",
        "    def fit(self, x, y, freq, alpha=1e-4, epochs=30, threshold_val=0.5, record_epoch=False):\n",
        "        self._inputs = x\n",
        "        self._tlen = len(y)\n",
        "        self._freq = freq\n",
        "        self._shape = 2\n",
        "        self.threshold_val = threshold_val\n",
        "\n",
        "        # return sample of standard normal distribution\n",
        "        self._bias = tf.Variable(np.random.randn(1), name=\"Bias\")\n",
        "        self._weights = tf.Variable(np.random.randn(1, self._shape), name= \"Weight\")\n",
        "        self._tinit = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "        self._targets = y\n",
        "        self.alpha = alpha\n",
        "        self.epochs = epochs\n",
        "        self._repoch = record_epoch\n",
        "        self.__trainModel()\n",
        "        return\n",
        "\n",
        "    def __initSaver(self):\n",
        "        saver = tf.compat.v1.train.Saver()\n",
        "        self._saver = saver\n",
        "        self._sesspath = \"TSession\"\n",
        "        return\n",
        "\n",
        "    def threshold(self, val):\n",
        "        if val >= self.threshold_val:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def predict(self, indata):\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            self._saver.restore(sess, save_path=self._sesspath)\n",
        "            indata = self.__transform_data(indata)\n",
        "            return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(indata, self._weights, transpose_b=True), self._bias)))\n",
        "        \n",
        "        print(\"Failed to retreive session\")\n",
        "        return\n",
        "    \n",
        "    def getAccDets(self):\n",
        "        if self._repoch:\n",
        "            return self.__accurary_det\n",
        "        return\n",
        "    \n",
        "    def __trainModel(self):\n",
        "        if self._repoch:\n",
        "            self._err = []\n",
        "            self._precs = []\n",
        "            self.__accurary_det = []\n",
        "\n",
        "        # model of Logistic Regression of tensorflow\n",
        "        logi = tf.nn.sigmoid(tf.add(tf.matmul(self._inputs, b=self._weights, transpose_b=True), self._bias))\n",
        "        err = tf.nn.sigmoid_cross_entropy_with_logits(logits=logi, labels=self._targets)\n",
        "        temp = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=self.alpha).minimize(err)\n",
        "        self.__initSaver()\n",
        "        with tf.compat.v1.Session() as sess:\n",
        "            sess.run(self._tinit)\n",
        "            print(f\"Bias: {sess.run(self._bias)}\")\n",
        "            print(f\"Weights: {sess.run(self._weights)}\")\n",
        "\n",
        "            for epoch in range(self.epochs):\n",
        "                sess.run(temp)\n",
        "                __preds = sess.run(logi)\n",
        "                acc = ((__preds==self._targets).sum()) / self._tlen\n",
        "                if self._repoch:\n",
        "                    self.__accurary_det.append(acc)\n",
        "                if epoch%10 == 0:\n",
        "                    print(f\"Accuracy: {acc}\")\n",
        "            self._saver.save(sess, self._sesspath)\n",
        "        return"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhjWovxV091s",
        "outputId": "73c2e42e-dea5-4bc3-be67-77bda8a55e93"
      },
      "source": [
        "X = np.zeros((len(x_train), 2))\n",
        "for i in range(len(x_train)):\n",
        "    X[i, :] = extract_features(x_train[i], freqs)\n",
        "\n",
        "model = TweetLogisticRegressionModel()\n",
        "model.fit(x=X, y= y_train, freq=freqs, alpha=0.01, record_epoch=True, epochs=100)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bias: [-0.68617635]\n",
            "Weights: [[0.71908574 0.96995028]]\n",
            "Accuracy: 0.49625\n",
            "Accuracy: 0.496375\n",
            "Accuracy: 0.4965\n",
            "Accuracy: 0.4965\n",
            "Accuracy: 0.4965\n",
            "Accuracy: 0.496875\n",
            "Accuracy: 0.496875\n",
            "Accuracy: 0.496875\n",
            "Accuracy: 0.496875\n",
            "Accuracy: 0.496875\n"
          ]
        }
      ]
    }
  ]
}